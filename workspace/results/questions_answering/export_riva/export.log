[NeMo W 2023-05-25 21:01:55 experimental:27] Module <class 'nemo.collections.nlp.data.language_modeling.megatron.megatron_batch_samplers.MegatronPretrainingRandomBatchSampler'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2023-05-25 21:01:56 experimental:27] Module <class 'nemo.collections.nlp.models.text_normalization_as_tagging.thutmose_tagger.ThutmoseTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.
[NeMo W 2023-05-25 21:01:56 nemo_logging:349] <frozen conv_ai.nlp.question_answering.scripts.export>:116: UserWarning: 
    'export.yaml' is validated against ConfigStore schema with the same name.
    This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.
    See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.
    
[NeMo I 2023-05-25 21:01:56 <frozen core:20] Experiment configuration:
    restore_from: /results/questions_answering/train/checkpoints/trained-model.tlt
    export_to: qa-model.riva
    export_format: RIVA
    exp_manager:
      task_name: export
      explicit_log_dir: /results/questions_answering/export_riva
    encryption_key: '*******'
    binary_type: ???
    binary_q_bits: 0
    binary_b_bits: 0
    binary_a_bits: 0
    
[NeMo W 2023-05-25 21:01:56 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:91: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
      rank_zero_warn(
    
[NeMo W 2023-05-25 21:01:58 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1823: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=8)`.
      rank_zero_warn(
    
[NeMo I 2023-05-25 21:02:00 tokenizer_utils:130] Getting HuggingFace AutoTokenizer with pretrained_model_name: bert-base-uncased, vocab_file: /root/.cache/huggingface/nemo_nlp_tmp/ae0d012864bdb2474ba67537c3f5e0fa/vocab.txt, merges_files: None, special_tokens_dict: {}, and use_fast: False
[NeMo W 2023-05-25 21:02:01 modelPT:217] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.
[NeMo W 2023-05-25 21:02:04 modelPT:217] You tried to register an artifact under config key=language_model.config_file but an artifact for it has already been registered.
[NeMo I 2023-05-25 21:02:08 <frozen conv_ai.nlp.question_answering.scripts:82] Model restored from '/results/questions_answering/train/checkpoints/trained-model.tlt'
[NeMo I 2023-05-25 21:02:29 <frozen conv_ai.nlp.question_answering.scripts:104] Experiment logs saved to '/results/questions_answering/export_riva'
[NeMo I 2023-05-25 21:02:29 <frozen conv_ai.nlp.question_answering.scripts:105] Exported model to '/results/questions_answering/export_riva/qa-model.riva'
[NeMo I 2023-05-25 21:02:31 <frozen conv_ai.nlp.question_answering.scripts:112] Exported model is compliant with Riva
